{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ler breast cancer dataset\n",
    "\n",
    "Esse dataset contém dados de pacientes com tumores de mama, você tentará prever se um tumor é maligno ou benigno de acordo com a espessura do tumor. No gráfico abaixo você pode visualizar os tipos de tumores (1 = maligno, 0 = benigno) por categoria de espessura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f355c888080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\",\n",
    "                header=None)\n",
    "\n",
    "y = np.array(x[10])\n",
    "y[y==2] = 0\n",
    "y[y==4] = 1\n",
    "x = np.array(x[1])\n",
    "\n",
    "plt.plot(x,y,'ro', alpha = 0.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código para o gradient descent\n",
    "\n",
    "Conforme implementado na Semana 1, o código abaixo é responsável por definir as funções do gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logverossimil(x, y, w, b):\n",
    "    return (b * np.sum(y)) + (w * np.sum(x * y)) - (np.sum(np.log(1 + np.exp(b + w *x))))\n",
    "\n",
    "\n",
    "def plot_logverossimil(wmin,wmax,bmin,bmax, history = None, x=x, y=y):\n",
    "    wrange = np.arange(wmin,wmax,0.1)\n",
    "    brange = np.arange(bmin,bmax,0.1)\n",
    "    v = []\n",
    "    for w in wrange:\n",
    "        vaux = []\n",
    "        for b in brange:\n",
    "            vaux.append(logverossimil(x,y,w,b))\n",
    "        v.append(np.array(vaux))\n",
    "    v = np.array(v)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    CS = ax.contour(brange, wrange, v)\n",
    "    plt.xlabel(\"b\", fontsize = 16)\n",
    "    plt.ylabel(\"w\", fontsize = 16)\n",
    "    ax.clabel(CS, inline=1, fontsize=11, fmt='%1.f')\n",
    "    if not(history is None):\n",
    "        plt.plot(history[:,0],history[:,1],color='k',linestyle='-', linewidth=0.5, marker='o',alpha=0.5,markersize=4)\n",
    "    plt.title (\"Log-verossimilhança por W e b\")\n",
    "    plt.show()\n",
    "\n",
    "def derivatives(x,y,w,b):\n",
    "    derivs = np.zeros(2)\n",
    "    derivs[0] = np.sum(y-1/(1+np.exp(-(w*x+b))))\n",
    "    derivs[1] = np.sum(x*(y-1/(1+np.exp(-(w*x+b)))))\n",
    "    return derivs\n",
    "\n",
    "def gradient_descent(x, y, b_init, w_init, learning_rate):\n",
    "    params = np.array([b_init, w_init]) \n",
    "    past_params = np.array([10,10])\n",
    "\n",
    "    history = [params] #armazene o histórico de valores nesse vetor\n",
    "    iters = 0\n",
    "    diff = np.array([10,10]) #para ser usado na condição de parada\n",
    "\n",
    "    while iters < 3000 and np.max(diff) > 1e-5: #condição de parada do algoritmo (n de iterações e convergência)\n",
    "        b, w = params[0], params[1]\n",
    "        past_params = params\n",
    "        params = params + learning_rate*derivatives(x,y,w,b)\n",
    "        diff = abs(params - past_params)\n",
    "        history.append(params)\n",
    "        iters += 1\n",
    "\n",
    "    history = np.array(history)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 01:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização\n",
    "No código abaixo iremos visualizar o efeito da normalização dos inputs no gradient descent. Execute o algoritmo nos dados não normalizados, depois normalize os dados e execute-o novamente. Imprima o número de iterações necessários para a convergência das duas execuções.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = #execute o gradient descent com os dados não normalizados\n",
    "plot_logverossimil(np.min(history[:,1]),np.max(history[:,1]),np.min(history[:,0]),np.max(history[:,0]), history)\n",
    "\n",
    "x_norm = #normalize os dados\n",
    "\n",
    "history_norm = #execute o gradient descent com os dados normalizados\n",
    "plot_logverossimil(np.min(history_norm[:,1]),np.max(history_norm[:,1]),np.min(history_norm[:,0]),np.max(history_norm[:,0]), history_norm)\n",
    "\n",
    "#imprima o número de iterações das duas execuções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 02:\n",
    "Implemente a função do gradient descent com momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente com momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_momentum(x, y, b_init, w_init, learning_rate, beta):\n",
    "    #Implemente aqui a função do gradient com momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = gradient_descent(x,y, 0,0, 0.001)\n",
    "plot_logverossimil(np.min(history[:,1]),np.max(history[:,1]),np.min(history[:,0]),np.max(history[:,0]), history)\n",
    "\n",
    "history_momentum = gradient_descent_momentum(x,y,0,0, 0.005, 0.9)\n",
    "plot_logverossimil(np.min(history_momentum[:,1]),np.max(history_momentum[:,1]),np.min(history_momentum[:,0]),np.max(history_momentum[:,0]), history_momentum, x=x)\n",
    "\n",
    "print(\"num iterations gd:\", len(history))\n",
    "print(\"num iterations gd w/ momentum:\", len(history_momentum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 03:\n",
    "Implemente a função do gradient descent RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_rmsprop(x, y, b_init, w_init, learning_rate, beta):\n",
    "    #Implemente aqui a função do RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rmsprop = gradient_descent_rmsprop(x,y,0,0, 0.01, 0.999)\n",
    "plot_logverossimil(np.min(history_rmsprop[:,1]),np.max(history_rmsprop[:,1]),np.min(history_rmsprop[:,0]),np.max(history_momentum[:,0]), history_rmsprop, x=x)\n",
    "\n",
    "print(\"num iterations gd w/ rmsprop:\", len(history_rmsprop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 04:\n",
    "Implemente a função do gradient descent Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_adam(x, y, b_init, w_init, learning_rate, beta1, beta2):\n",
    "    #Implemente aqui a função do Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_adam = gradient_descent_adam(x,y,0,0, 0.05, 0.9, 0.999)\n",
    "plot_logverossimil(np.min(history_adam[:,1]),np.max(history_adam[:,1]),np.min(history_adam[:,0]),np.max(history_adam[:,0]), history_adam, x=x)\n",
    "\n",
    "print(\"num iterations gd w/ adam:\", len(history_adam))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}