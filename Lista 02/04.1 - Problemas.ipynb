{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"04.1 - Problemas.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["Pd8hG7HCDUib","IDaRVNq1aMpm"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ed03SC1Jm9Yy"},"source":["## Aprendizado Profundo - UFMG\n","\n","## Problemas\n","\n","Como vimos acima, há muitos passos na criação e definição de uma nova rede neural.\n","A grande parte desses ajustes dependem diretamente do problemas.\n","\n","Abaixo, listamos alguns problemas. Todos os problemas e datasets usados vem do [Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/datasets.php).\n","\n","\n","**Seu objetivo é determinar e implementar um modelo para cada problema.**\n","\n","Isso inclui definir uma arquitetura (por enquanto usando somente camadas [Densas](https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Dense), porém podemos variar as ativações -- [Sigmoid](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.sigmoid), [Tanh](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.tanh), [ReLU](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.Symbol.relu), [LeakyReLU, ELU, SeLU, PReLU, RReLU](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.LeakyReLU)), uma função de custo ( [L1](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L2Loss), [L2](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.L1Loss),[ Huber](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HuberLoss), [*Cross-Entropy*](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss), [Hinge](https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.HingeLoss)), e um algoritmo de otimização ([SGD](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.SGD), [Momentum](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.SGD), [RMSProp](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.RMSProp), [Adam](https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.Adam)).\n","\n","A leitura do dado assim como a função de treinamento já estão implementados."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gp6CwWnFnTwb"},"source":["Esse pequeno bloco de código abaixo é usado somente para instalar o MXNet para CUDA 10. Execute esse bloco somente uma vez e ignore possíveis erros levantados durante a instalação.\n","\n","**ATENÇÃO: a alteração deste bloco pode implicar em problemas na execução dos blocos restantes!**"]},{"cell_type":"code","metadata":{"id":"PSl4OM6mgBxk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":315},"outputId":"5d09ce0a-75da-40e8-cee3-7fbbf3e70a8f","executionInfo":{"status":"ok","timestamp":1568309114748,"user_tz":180,"elapsed":49205,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}}},"source":["! pip install mxnet-cu100"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting mxnet-cu100\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/d3/e939814957c2f09ecdd22daa166898889d54e5981e356832425d514edfb6/mxnet_cu100-1.5.0-py2.py3-none-manylinux1_x86_64.whl (540.1MB)\n","\u001b[K     |████████████████████████████████| 540.1MB 35kB/s \n","\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet-cu100)\n","  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (1.16.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu100) (2.21.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2019.6.16)\n","Installing collected packages: graphviz, mxnet-cu100\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-cu100-1.5.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tjtbF5EKgBx3","colab_type":"text"},"source":["# Preâmbulo"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XW-VATPAldgt","colab":{}},"source":["# imports basicos\n","\n","from mxnet import autograd\n","from mxnet import gluon\n","from mxnet import init\n","from mxnet import nd\n","\n","from mxnet.gluon import data as gdata\n","from mxnet.gluon import loss as gloss\n","from mxnet.gluon import nn\n","from mxnet.gluon import utils as gutils\n","\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","\n","import mxnet as mx\n","import numpy as np\n","\n","import os\n","import sys\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"24IsujrWgByI","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.ion()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKuweg6fgByS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8be37892-df7c-4743-e9e1-1ce48f9072d5","executionInfo":{"status":"ok","timestamp":1568309120123,"user_tz":180,"elapsed":43742,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}}},"source":["# Tenta encontrar GPU\n","def try_gpu():\n","    try:\n","        ctx = mx.gpu()\n","        _ = nd.zeros((1,), ctx=ctx)\n","    except mx.base.MXNetError:\n","        ctx = mx.cpu()\n","    return ctx\n","\n","ctx = try_gpu()\n","ctx"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["gpu(0)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8oSVf8u1Oi1m","colab":{}},"source":["# funções básicas\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","\n","def _get_batch(batch, ctx):\n","    \"\"\"Return features and labels on ctx.\"\"\"\n","    features, labels = batch\n","    if labels.dtype != features.dtype:\n","        labels = labels.astype(features.dtype)\n","    return (gutils.split_and_load(features, ctx),\n","            gutils.split_and_load(labels, ctx), features.shape[0])\n","\n","# Função usada para calcular acurácia\n","def evaluate_accuracy(data_iter, net, loss, ctx=[mx.cpu()]):\n","    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n","    if isinstance(ctx, mx.Context):\n","        ctx = [ctx]\n","    acc_sum, n, l = nd.array([0]), 0, 0\n","    for batch in data_iter:\n","        features, labels, _ = _get_batch(batch, ctx)\n","        for X, y in zip(features, labels):\n","            # X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            y = y.astype('float32')\n","            y_hat = net(X)\n","            l += loss(y_hat, y).sum()\n","            acc_sum += (y_hat.argmax(axis=1) == y).sum().copyto(mx.cpu())\n","            n += y.size\n","        acc_sum.wait_to_read()\n","    return acc_sum.asscalar() / n, l.asscalar() / n\n","  \n","    \n","# Função usada no treinamento e validação da rede\n","def train_validate(net, train_iter, test_iter, batch_size, trainer, loss, ctx,\n","                   num_epochs, type='regression'):\n","    print('training on', ctx)\n","    for epoch in range(num_epochs):\n","        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n","        for X, y in train_iter:\n","            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n","            with autograd.record():\n","                y_hat = net(X)\n","                l = loss(y_hat, y).sum()\n","            l.backward()\n","            trainer.step(batch_size)\n","            y = y.astype('float32')\n","            train_l_sum += l.asscalar()\n","            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n","            n += y.size\n","        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss, ctx)\n","        if type == 'regression':\n","            print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n","                    % (epoch + 1, train_l_sum / n, test_loss, time.time() - start))\n","        else:\n","            print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, ' \\\n","                  'test acc %.3f, time %.1f sec' % \\\n","                  (epoch + 1, train_l_sum / n, train_acc_sum / n, test_loss, test_acc, time.time() - start))\n","          \n","        \n","# funcao usada para teste\n","def test(net, test_iter):\n","    print('testing on', ctx)\n","    first = True\n","    for X in test_iter:\n","        X = X.as_in_context(ctx)\n","        y_hat = net(X)\n","        if first is True:\n","            pred_logits = y_hat\n","            pred_labels = y_hat.argmax(axis=1)\n","            first = False\n","        else:\n","            pred_logits = nd.concat(pred_logits, y_hat, dim=0)\n","            pred_labels = nd.concat(pred_labels, y_hat.argmax(axis=1), dim=0)\n","\n","    return pred_logits.asnumpy(), pred_labels.asnumpy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y0m-qic-0Wnl"},"source":["## Problema 1\n","\n","Neste problema, você receberá 7 *features* extraídas de poços de petróleo ('BRCALI', 'BRDENS', 'BRDTP', 'BRGR', 'BRNEUT', 'BRRESC', 'BRRESP') e deve predizer o tipo de rocha."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"U64ACnJoGsDv"},"source":["### Treino e Validação\n","\n","Primeiro, vamos modelar uma rede neural e treiná-la.\n","Usamos o dado de treino carregado no próximo bloco para convergir o modelo e o dado de validação para avaliar quão bom ele estão. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AUYOPZYH0Ztc","colab":{"base_uri":"https://localhost:8080/","height":489},"outputId":"61e450fb-5691-453f-9de5-ca2e01d25ef9","executionInfo":{"status":"ok","timestamp":1568309125283,"user_tz":180,"elapsed":39412,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}}},"source":["# download do dataset\n","!wget https://www.dropbox.com/s/ujnqxh6l43tlbdi/poco_1.prn\n","X = np.loadtxt('poco_1.prn', skiprows=11, usecols=(1,2,3,4,5,6,7), dtype=np.float32)\n","y = np.loadtxt('poco_1.prn', skiprows=11, usecols=8, dtype=np.str)\n","print(y)\n","print(set(y))\n","le = preprocessing.LabelEncoder()\n","le.fit(list(set(y)))\n","y_t = le.transform(y)\n","\n","print(X[0, :])\n","print(y[0], y_t[0])\n","print(y[960], y_t[960])\n","train_features, test_features, train_labels, test_labels = train_test_split(X, y_t, test_size=0.33)\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","  \n","batch_size = 10\n","train_iter = load_array(train_features, train_labels, batch_size)\n","test_iter = load_array(test_features, test_labels, batch_size, False)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--2019-09-12 17:26:28--  https://www.dropbox.com/s/ujnqxh6l43tlbdi/poco_1.prn\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/ujnqxh6l43tlbdi/poco_1.prn [following]\n","--2019-09-12 17:26:29--  https://www.dropbox.com/s/raw/ujnqxh6l43tlbdi/poco_1.prn\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc0414dd52f6561961141a4e9028.dl.dropboxusercontent.com/cd/0/inline/AoZq9yVFkbZXMnx4-m2sELhAztYEH0JPRk2Czk4I-n9QtdKL0wzRaqr9S0iQhYWaioVgsSV7mOC3TaEJilzYarPaVoLdtVL0NhUFqA9CtOlkVA/file# [following]\n","--2019-09-12 17:26:30--  https://uc0414dd52f6561961141a4e9028.dl.dropboxusercontent.com/cd/0/inline/AoZq9yVFkbZXMnx4-m2sELhAztYEH0JPRk2Czk4I-n9QtdKL0wzRaqr9S0iQhYWaioVgsSV7mOC3TaEJilzYarPaVoLdtVL0NhUFqA9CtOlkVA/file\n","Resolving uc0414dd52f6561961141a4e9028.dl.dropboxusercontent.com (uc0414dd52f6561961141a4e9028.dl.dropboxusercontent.com)... 162.125.82.6, 2620:100:6032:6::a27d:5206\n","Connecting to uc0414dd52f6561961141a4e9028.dl.dropboxusercontent.com (uc0414dd52f6561961141a4e9028.dl.dropboxusercontent.com)|162.125.82.6|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 216320 (211K) [text/plain]\n","Saving to: ‘poco_1.prn’\n","\n","poco_1.prn          100%[===================>] 211.25K   762KB/s    in 0.3s    \n","\n","2019-09-12 17:26:31 (762 KB/s) - ‘poco_1.prn’ saved [216320/216320]\n","\n","['FLH' 'FLH' 'FLH' ... 'FLH' 'FLH' 'FLH']\n","{'FLH', 'ARN_MAC'}\n","[ 12.6936245   2.519692   89.70941   146.68591    27.542257    1.5375785\n","   1.2207866]\n","FLH 1\n","ARN_MAC 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pzVzMGeLgE0p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6476c44b-da71-4e4c-de9f-c6ba9bb1f038","executionInfo":{"status":"ok","timestamp":1568309201395,"user_tz":180,"elapsed":113344,"user":{"displayName":"Thiago Coutinho","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBAUarM7s4ca1oZAqgEMocrKktkBMLH_dbP07jNlA=s64","userId":"05429619374370003317"}}},"source":["net = nn.Sequential()\n","net.add(nn.Dense(64, activation='relu'),\n","        nn.Dropout(0.2),\n","        nn.Dense(32, activation='relu'),\n","        nn.Dense(16, activation='relu'),\n","        nn.Dense(2))\n","\n","weight_decay = 0.01\n","\n","net.initialize(init.Normal(sigma=0.01), ctx=ctx)\n","num_epochs = 100\n","l2_norm = gloss.SoftmaxCrossEntropyLoss()\n","trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate' : 0.0001, 'wd' : weight_decay})\n","train_validate(net, train_iter, test_iter, batch_size, trainer, l2_norm, ctx, num_epochs, type='classification')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["training on gpu(0)\n","epoch 1, train loss 0.6917, train acc 0.650, test loss 0.6902, test acc 0.665, time 0.9 sec\n","epoch 2, train loss 0.6872, train acc 0.666, test loss 0.6806, test acc 0.665, time 0.8 sec\n","epoch 3, train loss 0.6648, train acc 0.666, test loss 0.6472, test acc 0.665, time 0.8 sec\n","epoch 4, train loss 0.6124, train acc 0.666, test loss 0.5574, test acc 0.665, time 0.8 sec\n","epoch 5, train loss 0.4943, train acc 0.666, test loss 0.4697, test acc 0.665, time 0.8 sec\n","epoch 6, train loss 0.4399, train acc 0.690, test loss 0.4330, test acc 0.848, time 0.8 sec\n","epoch 7, train loss 0.4071, train acc 0.877, test loss 0.4042, test acc 0.873, time 0.8 sec\n","epoch 8, train loss 0.3744, train acc 0.889, test loss 0.3661, test acc 0.891, time 0.8 sec\n","epoch 9, train loss 0.3384, train acc 0.899, test loss 0.3274, test acc 0.905, time 0.8 sec\n","epoch 10, train loss 0.3063, train acc 0.905, test loss 0.3063, test acc 0.900, time 0.8 sec\n","epoch 11, train loss 0.2813, train acc 0.905, test loss 0.3046, test acc 0.894, time 0.7 sec\n","epoch 12, train loss 0.2726, train acc 0.902, test loss 0.2843, test acc 0.909, time 0.7 sec\n","epoch 13, train loss 0.2664, train acc 0.914, test loss 0.2826, test acc 0.909, time 0.8 sec\n","epoch 14, train loss 0.2616, train acc 0.904, test loss 0.2870, test acc 0.902, time 0.8 sec\n","epoch 15, train loss 0.2603, train acc 0.911, test loss 0.2843, test acc 0.907, time 0.7 sec\n","epoch 16, train loss 0.2574, train acc 0.907, test loss 0.2787, test acc 0.909, time 0.7 sec\n","epoch 17, train loss 0.2464, train acc 0.914, test loss 0.3013, test acc 0.898, time 0.8 sec\n","epoch 18, train loss 0.2446, train acc 0.916, test loss 0.2755, test acc 0.909, time 0.8 sec\n","epoch 19, train loss 0.2384, train acc 0.914, test loss 0.2799, test acc 0.909, time 0.7 sec\n","epoch 20, train loss 0.2392, train acc 0.916, test loss 0.2811, test acc 0.907, time 0.8 sec\n","epoch 21, train loss 0.2385, train acc 0.910, test loss 0.2804, test acc 0.907, time 0.8 sec\n","epoch 22, train loss 0.2389, train acc 0.909, test loss 0.2733, test acc 0.911, time 0.8 sec\n","epoch 23, train loss 0.2373, train acc 0.917, test loss 0.2859, test acc 0.905, time 0.7 sec\n","epoch 24, train loss 0.2331, train acc 0.917, test loss 0.2712, test acc 0.912, time 0.8 sec\n","epoch 25, train loss 0.2309, train acc 0.925, test loss 0.2699, test acc 0.914, time 0.8 sec\n","epoch 26, train loss 0.2313, train acc 0.919, test loss 0.2735, test acc 0.911, time 0.8 sec\n","epoch 27, train loss 0.2310, train acc 0.918, test loss 0.2699, test acc 0.916, time 0.7 sec\n","epoch 28, train loss 0.2358, train acc 0.912, test loss 0.2745, test acc 0.909, time 0.8 sec\n","epoch 29, train loss 0.2325, train acc 0.919, test loss 0.2737, test acc 0.907, time 0.7 sec\n","epoch 30, train loss 0.2314, train acc 0.917, test loss 0.2726, test acc 0.912, time 0.8 sec\n","epoch 31, train loss 0.2297, train acc 0.920, test loss 0.2688, test acc 0.916, time 0.8 sec\n","epoch 32, train loss 0.2297, train acc 0.923, test loss 0.2683, test acc 0.916, time 0.8 sec\n","epoch 33, train loss 0.2301, train acc 0.922, test loss 0.2703, test acc 0.912, time 0.8 sec\n","epoch 34, train loss 0.2302, train acc 0.923, test loss 0.2731, test acc 0.911, time 0.8 sec\n","epoch 35, train loss 0.2266, train acc 0.914, test loss 0.2692, test acc 0.916, time 0.8 sec\n","epoch 36, train loss 0.2243, train acc 0.922, test loss 0.2844, test acc 0.911, time 0.8 sec\n","epoch 37, train loss 0.2278, train acc 0.916, test loss 0.2713, test acc 0.912, time 0.8 sec\n","epoch 38, train loss 0.2315, train acc 0.918, test loss 0.2684, test acc 0.916, time 0.7 sec\n","epoch 39, train loss 0.2244, train acc 0.926, test loss 0.2712, test acc 0.911, time 0.8 sec\n","epoch 40, train loss 0.2211, train acc 0.918, test loss 0.2697, test acc 0.916, time 0.8 sec\n","epoch 41, train loss 0.2295, train acc 0.922, test loss 0.2794, test acc 0.912, time 0.8 sec\n","epoch 42, train loss 0.2262, train acc 0.915, test loss 0.2737, test acc 0.911, time 0.7 sec\n","epoch 43, train loss 0.2254, train acc 0.919, test loss 0.2789, test acc 0.912, time 0.8 sec\n","epoch 44, train loss 0.2215, train acc 0.918, test loss 0.2798, test acc 0.912, time 0.8 sec\n","epoch 45, train loss 0.2233, train acc 0.921, test loss 0.2781, test acc 0.912, time 0.8 sec\n","epoch 46, train loss 0.2291, train acc 0.922, test loss 0.2689, test acc 0.918, time 0.7 sec\n","epoch 47, train loss 0.2257, train acc 0.922, test loss 0.2699, test acc 0.914, time 0.8 sec\n","epoch 48, train loss 0.2202, train acc 0.927, test loss 0.2711, test acc 0.912, time 0.8 sec\n","epoch 49, train loss 0.2326, train acc 0.921, test loss 0.2732, test acc 0.911, time 0.8 sec\n","epoch 50, train loss 0.2311, train acc 0.914, test loss 0.2706, test acc 0.911, time 0.7 sec\n","epoch 51, train loss 0.2232, train acc 0.918, test loss 0.2675, test acc 0.918, time 0.8 sec\n","epoch 52, train loss 0.2239, train acc 0.917, test loss 0.2717, test acc 0.911, time 0.8 sec\n","epoch 53, train loss 0.2208, train acc 0.918, test loss 0.2714, test acc 0.914, time 0.8 sec\n","epoch 54, train loss 0.2240, train acc 0.920, test loss 0.2700, test acc 0.918, time 0.7 sec\n","epoch 55, train loss 0.2275, train acc 0.925, test loss 0.2770, test acc 0.912, time 0.8 sec\n","epoch 56, train loss 0.2246, train acc 0.920, test loss 0.2694, test acc 0.914, time 0.8 sec\n","epoch 57, train loss 0.2256, train acc 0.918, test loss 0.2686, test acc 0.916, time 0.7 sec\n","epoch 58, train loss 0.2263, train acc 0.923, test loss 0.2744, test acc 0.911, time 0.7 sec\n","epoch 59, train loss 0.2173, train acc 0.923, test loss 0.2702, test acc 0.914, time 0.8 sec\n","epoch 60, train loss 0.2221, train acc 0.924, test loss 0.2741, test acc 0.911, time 0.8 sec\n","epoch 61, train loss 0.2229, train acc 0.924, test loss 0.2737, test acc 0.911, time 0.8 sec\n","epoch 62, train loss 0.2243, train acc 0.921, test loss 0.2732, test acc 0.911, time 0.8 sec\n","epoch 63, train loss 0.2259, train acc 0.925, test loss 0.2709, test acc 0.911, time 0.7 sec\n","epoch 64, train loss 0.2256, train acc 0.922, test loss 0.2713, test acc 0.911, time 0.7 sec\n","epoch 65, train loss 0.2195, train acc 0.925, test loss 0.2804, test acc 0.914, time 0.8 sec\n","epoch 66, train loss 0.2247, train acc 0.920, test loss 0.2670, test acc 0.916, time 0.8 sec\n","epoch 67, train loss 0.2230, train acc 0.924, test loss 0.2774, test acc 0.912, time 0.8 sec\n","epoch 68, train loss 0.2187, train acc 0.919, test loss 0.2795, test acc 0.912, time 0.8 sec\n","epoch 69, train loss 0.2250, train acc 0.918, test loss 0.2709, test acc 0.916, time 0.7 sec\n","epoch 70, train loss 0.2148, train acc 0.922, test loss 0.2778, test acc 0.911, time 0.8 sec\n","epoch 71, train loss 0.2232, train acc 0.925, test loss 0.2758, test acc 0.911, time 0.8 sec\n","epoch 72, train loss 0.2224, train acc 0.922, test loss 0.2766, test acc 0.912, time 0.8 sec\n","epoch 73, train loss 0.2214, train acc 0.926, test loss 0.2844, test acc 0.916, time 0.8 sec\n","epoch 74, train loss 0.2243, train acc 0.920, test loss 0.2680, test acc 0.918, time 0.7 sec\n","epoch 75, train loss 0.2222, train acc 0.920, test loss 0.2734, test acc 0.911, time 0.8 sec\n","epoch 76, train loss 0.2245, train acc 0.920, test loss 0.2714, test acc 0.912, time 0.8 sec\n","epoch 77, train loss 0.2223, train acc 0.923, test loss 0.2690, test acc 0.916, time 0.7 sec\n","epoch 78, train loss 0.2207, train acc 0.923, test loss 0.2677, test acc 0.918, time 0.8 sec\n","epoch 79, train loss 0.2237, train acc 0.920, test loss 0.2737, test acc 0.911, time 0.8 sec\n","epoch 80, train loss 0.2179, train acc 0.921, test loss 0.2756, test acc 0.912, time 0.8 sec\n","epoch 81, train loss 0.2150, train acc 0.928, test loss 0.2763, test acc 0.911, time 0.8 sec\n","epoch 82, train loss 0.2219, train acc 0.923, test loss 0.2891, test acc 0.905, time 0.8 sec\n","epoch 83, train loss 0.2270, train acc 0.926, test loss 0.2759, test acc 0.911, time 0.8 sec\n","epoch 84, train loss 0.2197, train acc 0.924, test loss 0.2698, test acc 0.916, time 0.8 sec\n","epoch 85, train loss 0.2189, train acc 0.921, test loss 0.2705, test acc 0.916, time 0.8 sec\n","epoch 86, train loss 0.2251, train acc 0.919, test loss 0.2687, test acc 0.916, time 0.8 sec\n","epoch 87, train loss 0.2244, train acc 0.927, test loss 0.2769, test acc 0.912, time 0.7 sec\n","epoch 88, train loss 0.2192, train acc 0.929, test loss 0.2810, test acc 0.914, time 0.8 sec\n","epoch 89, train loss 0.2225, train acc 0.922, test loss 0.2769, test acc 0.912, time 0.8 sec\n","epoch 90, train loss 0.2232, train acc 0.922, test loss 0.2702, test acc 0.914, time 0.7 sec\n","epoch 91, train loss 0.2250, train acc 0.920, test loss 0.2799, test acc 0.914, time 0.8 sec\n","epoch 92, train loss 0.2228, train acc 0.922, test loss 0.2665, test acc 0.918, time 0.7 sec\n","epoch 93, train loss 0.2229, train acc 0.922, test loss 0.2654, test acc 0.918, time 0.8 sec\n","epoch 94, train loss 0.2214, train acc 0.922, test loss 0.2698, test acc 0.916, time 0.8 sec\n","epoch 95, train loss 0.2199, train acc 0.919, test loss 0.2725, test acc 0.911, time 0.8 sec\n","epoch 96, train loss 0.2209, train acc 0.922, test loss 0.2742, test acc 0.911, time 0.8 sec\n","epoch 97, train loss 0.2192, train acc 0.921, test loss 0.2778, test acc 0.912, time 0.8 sec\n","epoch 98, train loss 0.2231, train acc 0.924, test loss 0.2686, test acc 0.916, time 0.8 sec\n","epoch 99, train loss 0.2203, train acc 0.926, test loss 0.2739, test acc 0.911, time 0.8 sec\n","epoch 100, train loss 0.2206, train acc 0.922, test loss 0.2686, test acc 0.916, time 0.8 sec\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pd8hG7HCDUib"},"source":["## Problema 2\n","\n","Neste problema, você receberá várias *features* (como altura média, inclinação, etc) descrevendo uma região e o modelo deve predizer qual o tipo da região (floresta, montanha, etc)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IZcIXGqBDznB","colab":{}},"source":["!wget http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n","!gzip covtype.data.gz\n","data = np.genfromtxt('covtype.data', delimiter=',', dtype=np.float32)\n","\n","print(data.shape, data[0, :])\n","X, y = data[:, :-1], data[:, -1]\n","print(X.shape, X[0, :])\n","print(y.shape, y[0]k)\n","train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","  \n","batch_size = 100\n","train_iter = load_array(train_features, train_labels, batch_size)\n","test_iter = load_array(test_features, test_labels, batch_size, False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IDaRVNq1aMpm"},"source":["## Problema 3\n","\n","Neste problema, você receberá 90 *features* extraídas de diversas músicas (datadas de 1922 até 2011) e deve predizer o ano de cada música."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CWdBT3zhW_Y5","colab":{}},"source":["# download do dataset\n","!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n","!unzip YearPredictionMSD.txt.zip\n","data = np.genfromtxt('YearPredictionMSD.txt', delimiter=',', dtype=np.float32)\n","\n","print(data[0, :])\n","X, y = data[:, 1:], data[:, 0]\n","train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","def load_array(features, labels, batch_size, is_train=True):\n","    \"\"\"Construct a Gluon data loader\"\"\"\n","    dataset = gluon.data.ArrayDataset(features, labels)\n","    return gluon.data.DataLoader(dataset, batch_size, shuffle=is_train)\n","  \n","batch_size = 100\n","train_iter = load_array(train_features, train_labels, batch_size)\n","test_iter = load_array(test_features, test_labels, batch_size, False)"],"execution_count":0,"outputs":[]}]}